---
title: "A Kubernetes Journey"
date: "2025-01-26"
featured_image:
  caption: "Kubernetes logo"
  image: "kubernetes.png"
tags:
  - kubernetes
  - devops
  - ci-cd
  - nixos
  - fluxcd
toc: true
---

Bonne ann√©e √† tous ! üéâ

Pour ce premier article de 2025, nous allons parler DevOps, ou plus particuli√®rement Kubernetes. Cela sera l'occasion d'aborder des notions d'infrastructures,
de d√©ploiements continue, de gestion de secrets, etc.

<!--more-->

> [!WARNING]
>
> Avant d'aller plus loin, il est important de noter que je ne suis absolument pas un expert et il est fort probable que certaines de mes mani√®res de faire ne
> soient abssoluments pas optimales ou recommend√©s. N'h√©sitez pas √† me faire part de vos retours pour m'aider √† m'am√©liorer.

## Introduction

Cela fait maintenant plusieurs ann√©es que je poss√®de un home-server et il n'a cess√© d'√©voluer sur le plan technique ou sur celui de la puissance de calcul. Au
d√©but, il y a environs 9 ans, j'ai commenc√© avec un simple Raspberry Pi 2 sur lequel √©tait install√© Docker Swarm. Au fil tu temps, j'ai migr√© vers deux
Raspberry Pi 3 d'abord avec Docker Swarm puis avec Kubernetes, puis ajout√© un Raspberry Pi 4. Ensuite, il y a environ 2 ans, j'ai remplac√© ces Raspberry Pi par
deux Oranges Pi 5 √©quip√©s de disques NVME. Enfin, l'ann√©e derni√®re, j'ai int√©gr√© un PC neuf √† mon cluster Kubernetes. Et cette ann√©e, j'ai finalement d√©cid√© de
d√©brancher mes Oranges Pi pour ne converver que le PC.

## Le serveur

### Le mat√©riel

Le serveur est un PC mont√© moi-m√™me pour l'occasion. Il poss√®de les caract√©ristiques suivantes :

- Processeur : AMD Ryzen 5 5800G
- RAM : 32 Go (2x 16Go DDR5 5200MHz CL40)
- Carte m√®re: ASRock B650M-H/M.2+
- Alimentation: Cooler Master MWE Gold 550 FM (v2)
- Stockage: 1x SSD NVMe 512 Go
- Boitier: Fractal Design Core 1100

(Et oui, je me suis fais plaisir au moment de l'achat ! üòù)

### Le syst√®me d'exploitation

Pour le choix du syst√®me d'exploitation, j'ai opt√© pour un choix plut√¥t exotique √† premi√®re vue, mais assez int√©ressant : [NixOS](https://nixos.org/). NixOS est
une distribution Linux bas√©e sur Nix, un gestionnaire de paquets fonctionnel. Cela implique que la configuration du syst√®me est d√©clarative et que les paquets
sont isol√©s les uns des autres. En plus de me simplifier l'installation et paquets de bases, NixOS me permet aussi de rapidement configurer kubernetes (k3s) et
d'autres services avec assez peu de configuration. Cumul√© √† cela, j'ai activ√© la fonctionnalit√© de [Flake](https://wiki.nixos.org/wiki/Flakes) qui m'ouvre la
voie √† encore plus de capacit√© √† la configuration.

### Le d√©ploiement du syst√®me

Avant d'utiliser NixOS, toutes mes infrastructures utilisaient toujours une distribution bas√© sur Debian (Debian et Armbian plus pr√©cis√©ment) et toute
l'installation des outils se faisaient par le biais de scripts Ansible.

Ansible et Nix Flake sont deux outils ayant un objectif similaire : permettre un d√©ploiement automatis√© et reproductible. Il faut tout de m√™me noter que leur
proc√©d√© de fonctionnement est assez diff√©rent. Ansible est un outils de d√©ploiement avec une configuration dite "imp√©rative", c'est √† dire que l'on d√©crit les
√©tapes √† suivre pour arriver √† un √©tat donn√©. Nix Flake, quant √† lui, est un outil de d√©ploiement avec une configuration dite "d√©clarative", c'est √† dire que
l'on d√©crit l'√©tat final que l'on souhaite obtenir √† la fin.

Cette diff√©rence de paradigme est assez importante et c'est ce qui m'a pouss√© √† essayer NixOS. En effet, il faut savoir que jusqu'√† pr√©sent, je faisais tous mes
d√©ploiements via Ansible, mais au fur et √† mesure du temps, le co√ªt de maintenance des scripts devenait de plus en plus important en raison des mises √† jours
des nombreuses d√©pendences n√©cessaires (k3s, networking, adressage des IPs, etc.). Avec Nix Flake, j'ai n'ai plus trop √† me soucier de cela, et mieux encore,
des morceaux de mes configurations sont plus facilement r√©utilisables. (C'√©tait d√©j√† un peu le cas avec Ansible, mais pas autant).

> [!TIP]
>
> Pour ceux qui souhaitent en savoir plus sur NixOS, je vous recommande de lire le [manuel officiel](https://nixos.org/manual/nixos/stable/).

Voici un petit exemple de configuration Nix pour installer et configurer K3S :

```nix
## Il faut imaginer que ce fichier est import√© ailleur dans la configuration, ce qui permet de
## fournir le champ `pkgs` et les autres variables.
{
  pkgs,
  kubeconfigFile,
  tokenFile,
  # Initialize HA cluster using an embedded etcd datastore.
  # If you are configuring an HA cluster with an embedded etcd,
  # the 1st server must have `clusterInit = true`
  # and other servers must connect to it using `serverAddr`.
  #
  # this can be a domain name or an IP address(such as kube-vip's virtual IP)
  masterHost,
  clusterInit ? false,
  kubeletExtraArgs ? [],
  nodeLabels ? [],
  nodeTaints ? [],
  disableFlannel ? true,
  nodeIps,
  ...
}: let
  lib = pkgs.lib;
in {
  # Installation de paquet syst√®mes
  environment.systemPackages = with pkgs; [
    k9s # k9s est un outil TUI pour kubernetes
    kubectl # CLI officiel pour kubernetes
    kubecolor # coloration syntaxique pour kubectl (facultatif)
    istioctl # outil de ligne de commande pour Istio
    kubernetes-helm # outil de gestion de paquets pour kubernetes
    clusterctl # CLI pour piloter kubernetes depuis son api
  ];

  # Configuration du pare-feu
  networking.firewall = {
    allowedTCPPorts = [
      6443 # k3s: required so that pods can reach the API server (running on port 6443 by default)
    ];
    allowedUDPPorts = [
      8472 # k3s, flannel: required if using multi-node for inter-node networking
    ];
  };

  # Configuration de k3s
  services.k3s = {
    enable = true;
    # On h√©rite des variables d√©finies plus haut
    inherit package tokenFile clusterInit;
    serverAddr =
      if clusterInit
      then ""
      else "https://${masterHost}:6443";

    # On sp√©cifie le role du noeud
    role = "server";

    # On sp√©cifie les flags √† passer √† k3s
    # https://docs.k3s.io/cli/server
    extraFlags = let
      flagList =
        [
          "--write-kubeconfig=${kubeconfigFile}"
          "--write-kubeconfig-mode=644"
          "--kube-apiserver-arg='--allow-privileged=true'" # required by kubevirt
          "--data-dir /var/lib/rancher/k3s"
          "--etcd-expose-metrics=true"
          # to enable dual-stack, these flags are required
          # https://docs.k3s.io/networking/basic-network-options#dual-stack-ipv4--ipv6-networking
          "--cluster-cidr=10.42.0.0/16,2001:cafe:42::/56"
          "--service-cidr=10.43.0.0/16,2001:cafe:43::/112"
          # disable some features we don't need
          "--disable-helm-controller" # we use fluxcd instead
          "--disable=traefik" # deploy our own ingress controller instead
          "--disable=servicelb" # we use metallb instead
          "--tls-san=${masterHost}"
          "--node-ip=${lib.concatStringsSep "," nodeIps}"
        ]
        ++ (map (label: "--node-label=${label}") nodeLabels)
        ++ (map (taint: "--node-taint=${taint}") nodeTaints)
        ++ (map (arg: "--kubelet-arg=${arg}") kubeletExtraArgs)
        ++ (lib.optionals disableFlannel ["--flannel-backend=none"])
        ++ (lib.optionals (!disableFlannel) ["--flannel-ipv6-masq=true"]);
    in
      lib.concatStringsSep " " flagList;
  };
}
```

Comme vous pouvez le constater, la configuration est assez lisible dans l'ensemble. Le language Nix permet d'effecter des op√©rations assez complexes et ce
script en est un exemple. En une cinquantaine de lignes, nous avons une configuration compl√®te pour installer et configurer k3s, configurer le pare-feu, et
installer quelques outils suppl√©mentaires.

Si nous devions refaire la m√™me chose avec Ansible, cela nous prendrait beaucoup plus de lignes (pas tant que √ßa, mais tout de m√™me) et surtout, cela nous
demanderait de r√©fl√©chir en amont √† l'ordre d'ex√©cution des t√¢ches √† effectuer, √† la gestion des erreurs, etc.

> [!CAUTION]
>
> Attention tout de m√™me, NixOS n'est pas une solution miracle et il est possible de se retrouver dans des situations assez complexes si l'on ne fait pas
> attention. Il est donc important de bien comprendre les concepts de base avant de se lancer.
>
> Un autre point √† noter, c'est que NixOS, en plus de ne pas r√©specter les standards POSIX sur la structure du syst√®me de fichier (impliquant la mise en place
> de nombreux bricolages pour faire fonctionner certains programmes), demande une quantit√© de stockage plus importante que des distributions plus classiques.

## Pourquoi Kubernetes ?

Kubernetes est un orchestrateur de conteneurs open-source qui permet d'automatiser le d√©ploiement, la mise √† l'√©chelle et la gestion des applications
conteneuris√©es. Il est con√ßu pour g√©rer des applications conteneuris√©es sur un cluster de machines. Il a √©t√© con√ßu √† l'origine par Google et est maintenant
maintenu par la Cloud Native Computing Foundation.

Dans mon cas, Kubernetes me permet de d√©ployer et de faire la maintenance rapidement et facilement des applications sur mon serveur. Dans le cas o√π je d√©ploie
un grand nomrmes de services, avoir Kubernetes me simplifie grandement la gestion de tout cela.

N√©anmoins ! Qui dit simplicit√© au d√©ploiement, dit complexit√© √† la configuration de l'infrastructure. En effet, Kubernetes est un outil tr√®s puissant mais qui
peut √™tre difficile √† appr√©hender. Il est donc important de bien comprendre les concepts de base avant de se lancer.

## Processus de d√©ploiement

Jusque l√†, j'ai surtout parl√© de NixOS et de Kubernetes sans √™tre exactement rentr√© dans les d√©tails de _pourquoi_ c'est effectivement plus simple √† utiliser.
C'est ce que nous allons voir maintenant.

### Continuous Integration

Afin d'√©viter de d√©ployer des configurations cass√©es, j'ai mis en place des outils tel que des Github Actions et des hook `pre-commit` pour v√©rifier la syntaxe
des fichiers et les formater.

De cette mani√®re, je m'assure que les fichiers que je vais commiter sont corrects et que je n'aurais pas de surprise lors du d√©ploiement. (Cela ne garantit pas
que le d√©ploiement sera sans erreur, mais cela r√©duit les risques).

Pour cela, j'utilise 2 outils : `pre-commit-hooks` qui me fait toutes les v√©rifications avant le commit et `github actions` qui me fait les v√©rifications lors
du push sur la branche `main`. En compl√©ment, j'utilise [Renovate Bot](https://docs.renovatebot.com/) qui a pour r√¥le de v√©rifier les mises √† jours des
d√©pendances de mes projets. Tous les lundis, Renovate Bot va aller scanner le projet et chercher si des mises √† jours sont disponibles. Si c'est le cas, il va
ouvrir une PR pour me proposer d'appliquer ces mises √† jours.

### Continuous Deployment

Une fois un commit pouss√© sur Github, l'outil [FluxCD](https://fluxcd.io/) (install√© dans le cluster Kubernetes) va aller chercher les derniers commits
disponibles. Son r√¥le est de synchroniser les configurations pr√©sentent dans le d√©p√¥t avec ce qui est actuellement d√©ploiment en production. √Ä partir de l√†, il
va essayer de faire de son mieux pour appliquer les changements jusqu'√† atteindre l'√©tat attendu. Voici un exemple de configuration que FluxCD est capable
d'interpr√©ter pour faire la synchronisation:

```yaml
---
## On utilise une ressource custom fournit par FluxCD
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: website
  namespace: flux-system
spec:
  # On veut que cette configuration soit synchronis√© toutes les 30 minutes
  interval: 30m
  # Les valeurs `path` et `sourceRef` disent √† FluxCD d'aller chercher les configuration dans le
  # d√©p√¥t Git log√©es au chemin sp√©cifi√©. Dans mon cas, cela correspond au d√©p√¥t actuel dans le
  # dossier `./app` par rapport ce fichier de configuration
  path: ./k3s/fluxcd/apps/base/website/app
  sourceRef:
    kind: GitRepository
    name: flux-system
    namespace: flux-system
  # Si besoin, on d√©truit les ressources qui ne servent plus
  prune: true
  # On s'attend ce que le d√©ploiment soit fait sous 10 minutes
  timeout: 10m
  # On informe FluxCD qu'il peut statuer le d√©ploiement comme "pr√™t" uniquement lorsque le statut
  # de l'objet sp√©cifi√© est lui aussi statu√© comme "pr√™t".
  healthChecks:
    - kind: Deployment
      name: website
      namespace: website
```

Toutefois, il peut arriver qu'il ne soit pas capable re-synchroniser en raison d'erreur pouvant √™tre pr√©sente √† plusieurs niveaux (erreur de configuration,
image non-disponible, etc.). Quand cela arrive, FluxCD est capable de m'envoyer une notification directement sur Discord par le biais d'un webhook. Ce qui me
permet alors d'intervenir pour r√©gler le probl√®me.

### Sauvegarde des donn√©es

Ma pr√©c√©dente installation (2024) √©tait √©quip√© du syst√®me de sauvegarde de Longhorn. Sur le papier, le proc√©d√© fonctionnait bien, les volumes √©taient bien copi√©
sur mon NAS via NFS. Mais quand l'infrastructure √† l√¢ch√©, j'ai tout de m√™me perdu quelques donn√©es en chemin (par exemple, des donn√©es de base de donn√©e
Postgres qui n'ont pas support√© le processus). Heureusement pour moi, je n'ai perdu que tr√®s peu de donn√©e, mais cela m'a tout de m√™me fait r√©fl√©chir √† mon plan
de sauvegarde de mes donn√©es.

J'en suis donc arriv√© √† la conclusion de totalement abandonner le syst√®me de Longhorn et de les g√©rer ind√©pendament. J'ai donc r√©vis√© ma proc√©dure comme suis :

- Toutes mes sauvegarde passent maintenant par un serveur Minio S3 install√© sur le NAS.
- Les sauvegarde de base de donn√©es postgres (Cloud Native PG) sont maintenant g√©r√© directement par CNPG lui m√™me avec
  [son syst√®me automatis√©](https://cloudnative-pg.io/documentation/current/backup/). De cette mani√®re, je suis assur√© que les sauvegardes seront compatible sans
  d√©faillance li√© au processus de sauvegarde.
- Un de mes service n√©cessite une base de donn√©e MariaDB. Avant je ne d√©ployais qu'un simple pod et je sauvegardais le volume, mais l√† j'ai d√©cid√© d'aller plus
  loin en installant MariaDB Operator. Ce dernier, √† la mani√®re de CNPG, s'occupe lui aussi de sauvegarder les base de donn√©es sur S3.
- Pour finir, tous les autres volumes sont sauvegarder gr√¢ce √† [VolSync](https://volsync.readthedocs.io/), un outil sp√©cialis√© dans les plan de sauvegarde de
  volumes Kubernetes. Et vous savez quoi ? Il est aussi compatible S3 ! (sans grande surprise üòÑ)

Toutes les sauvegardes sont ex√©cut√©s de mani√®re hebdomadaires avec une r√©tention de 4 semaines avant suppression. Ce qui me laisse une marge plut√¥t confortable
en cas de panne du syst√®me.

```mermaid
architecture-beta
  group kubernetes(server)[Cluster K8S]
  group nas(server)[NAS sur le reseau local]
  
  service pgDb(database)[CNPG] in kubernetes
  service mariaDb(database)[MariaDB Operator] in kubernetes
  service volsync(disk)[VolSync] in kubernetes

  service minio(disk)[Minio S3] in nas

  junction junctionCenter in kubernetes

  pgDb:R -- L:junctionCenter
  mariaDb:T -- B:junctionCenter
  volsync:L -- R:junctionCenter

  junctionCenter:T --> B:minio
```

### Et les secrets dans tout √ßa ?

La gestion des secrets est le dernier point que nous allons aborder aujourd'hui. Bien que mon d√©p√¥t soit priv√©, je ne peux pas non plus me permettre de laisser
des donn√©es aussi sensible (cl√© api, mot de passe, etc.) accessible tel quel dans les fichiers. Pour rem√©dier √† ce probl√®me, je passe par l'outil
[SOPS](https://github.com/getsops/sops). Son fonctionnement est assez sommaire : il n√©cessite une cl√© GPG/SSH publique en entr√©e pour chiffrer les donn√©es, et
la cl√© GPG/SSH priv√©e associ√© pour d√©chifrer. √Ä partir de l√†, on peut rajouter plusieurs cl√©s correspondant √† plusieurs machine et il se d√©brouillera pour
chiffrer les donn√©es pour toutes les cl√©s.

Le programme est tr√®s bien int√©gr√© dans FluxCD et dans Nix (via [sops-nix](https://github.com/Mic92/sops-nix/)), ce qui en fait un outil id√©al pour g√©rer mes
donn√©es sensibles.

Par pr√©caution, j'ai quand m√™me un outil ([GitGuardian](https://www.gitguardian.com/)) dans ma CI Github pour scanner mes fichiers √† la recherche de potentiel
secret oubli√©. √âvid√©ment, si votre infrastructure est tr√®s sensible (dans le contexte en entreprise par exemple), il serait toujours possible d'installer ces
outils en interne _on-premise_.

## Conclusion

Cette petite aventure a √©t√© √† la fois instructive et amusante. J'ai pu tirer parti de la panne compl√®te de mon ancienne installation pour repartir de z√©ro et
r√©organiser toutes mes configurations. De plus, j'ai pu combler mes lacunes et tester de nouveaux outils tels que SOPS, FluxCD, NixOS et VolSync.

Apr√®s un peu plus d'un mois de fonctionnement, je n'ai toujours pas rencontr√© de probl√®me "majeur". On peut donc supposer que c'est migration est dans
l'ensemble un succ√®s. üòé

Si vous souhaitez avoir plus d'informations sur certains point, n'h√©sitez pas √† me contacter par le biais des
[Github Discussions](https://github.com/Bricklou/website/discussions). J'essaierais de vous r√©pondre et de mettre √† jours ce post avec les informations
compl√©mentaires.
